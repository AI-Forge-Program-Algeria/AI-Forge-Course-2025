{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Data"
      ],
      "metadata": {
        "id": "gEjv9uaiXCV9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCYgw-hPWrEV"
      },
      "source": [
        "## 01 - Reading Raw Files\n",
        "\n",
        "Python supports a number of standard and custom libraries to read files of all types into python variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NQxEoA5WrEa",
        "outputId": "38ab8607-700c-46c3-dea9-00879d438c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data read from file :  In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Read the file using standard python libaries\n",
        "with open(os.getcwd()+ \"/Spark-Course-Description.txt\", 'r') as fh:\n",
        "    filedata = fh.read()\n",
        "\n",
        "# Print first 200 characters in the file\n",
        "print(\"Data read from file : \", filedata[0:200] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq4b8XlnWrEf"
      },
      "source": [
        "## 02 - Reading using NLTK CorpusReader\n",
        "\n",
        "Read the same text file using a Corpus Reader\n",
        "\n",
        "NLTK supports multiple CorpusReaders depending upon the type of data source. Details available in http://www.nltk.org/howto/corpus.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoDhivzEWrEg",
        "outputId": "e26d24e0-3bb9-460b-82f4-61b075341869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: nltk\n",
            "Version: 3.9.1\n",
            "Summary: Natural Language Toolkit\n",
            "Home-page: https://www.nltk.org/\n",
            "Author: NLTK Team\n",
            "Author-email: nltk.team@gmail.com\n",
            "License: Apache License, Version 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: click, joblib, regex, tqdm\n",
            "Required-by: textblob\n"
          ]
        }
      ],
      "source": [
        "!pip show nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLUj2gbWZYDN",
        "outputId": "ac2b1bdb-e945-4ef5-c9c7-68fe7c64fa78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOyiDV6bWrEh",
        "outputId": "6822fd8f-80db-4b90-bde4-0b111825fd53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies. In this course, discover how to build big data pipelines around Apache Spark. Join Kumaran Ponnambalam as he takes you through how to make Apache Spark work with other big data technologies. He covers the basics of Apache Kafka Connect and how to integrate it with Spark for real-time streaming. In addition, he demonstrates how to use the various technologies to construct an end-to-end project that solves a real-world business problem.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download punkt package, used part of the other commands\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "# Define the sentence tokenizer\n",
        "sent_tokenizer = PunktSentenceTokenizer()\n",
        "\n",
        "# Read the file into a corpus. The same command can read an entire directory\n",
        "corpus = PlaintextCorpusReader(os.getcwd(), \"Spark-Course-Description.txt\", sent_tokenizer=sent_tokenizer)\n",
        "\n",
        "# Print raw contents of the corpus\n",
        "print(corpus.raw())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSLvGwIMWrEh"
      },
      "source": [
        "## 03 - Exploring the Corpus\n",
        "\n",
        "The corpus library supports a number of functions to extract words, paragraphs and sentences from the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwdchuAxWrEi",
        "outputId": "cadd3c54-fe4c-4012-e138-337a3a9ffd1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in this corpus :  ['Spark-Course-Description.txt']\n",
            "\n",
            "Total paragraphs in this corpus :  1\n",
            "\n",
            "The first paragraph in this corpus :  [['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.'], ['In', 'this', 'course', ',', 'discover', 'how', 'to', 'build', 'big', 'data', 'pipelines', 'around', 'Apache', 'Spark', '.'], ['Join', 'Kumaran', 'Ponnambalam', 'as', 'he', 'takes', 'you', 'through', 'how', 'to', 'make', 'Apache', 'Spark', 'work', 'with', 'other', 'big', 'data', 'technologies', '.'], ['He', 'covers', 'the', 'basics', 'of', 'Apache', 'Kafka', 'Connect', 'and', 'how', 'to', 'integrate', 'it', 'with', 'Spark', 'for', 'real', '-', 'time', 'streaming', '.'], ['In', 'addition', ',', 'he', 'demonstrates', 'how', 'to', 'use', 'the', 'various', 'technologies', 'to', 'construct', 'an', 'end', '-', 'to', '-', 'end', 'project', 'that', 'solves', 'a', 'real', '-', 'world', 'business', 'problem', '.']]\n",
            "\n",
            "Total sentences in this corpus :  5\n",
            "\n",
            "The first sentence :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.']\n",
            "\n",
            "Words in this corpus :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', ...]\n"
          ]
        }
      ],
      "source": [
        "# Extract the file IDs from the corpus\n",
        "print(\"Files in this corpus : \", corpus.fileids())\n",
        "\n",
        "# Extract paragraphs from the corpus\n",
        "paragraphs = corpus.paras()\n",
        "print(\"\\nTotal paragraphs in this corpus : \", len(paragraphs))\n",
        "print(\"\\nThe first paragraph in this corpus : \", paragraphs[0])\n",
        "\n",
        "# Extract sentences from the corpus\n",
        "sentences = corpus.sents()\n",
        "print(\"\\nTotal sentences in this corpus : \", len(sentences))\n",
        "print(\"\\nThe first sentence : \", sentences[0])\n",
        "\n",
        "# Extract words from the corpus\n",
        "print(\"\\nWords in this corpus : \", corpus.words() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoWybBgVWrEj"
      },
      "source": [
        "## 04 - Analyze the Corpus\n",
        "\n",
        "The NLTK library provides a number of functions to analyze the distributions and aggregates for data in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RelgkjOWrEk",
        "outputId": "93561487-67c2-406b-cb13-d315ddde88e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words in the corpus :  [('to', 8), ('data', 7), (',', 5), ('-', 5), ('how', 5), ('.', 5), ('and', 4), ('In', 3), ('big', 3), ('technologies', 3)]\n",
            "\n",
            "Distribution for \"Spark\" :  3\n"
          ]
        }
      ],
      "source": [
        "# Find the frequency distribution of words in the corpus\n",
        "course_freq_dist = nltk.FreqDist(corpus.words())\n",
        "\n",
        "# Print most commonly used words\n",
        "print(\"Top 10 words in the corpus : \", course_freq_dist.most_common(10))\n",
        "\n",
        "# Find the distribution for a specific word\n",
        "print(\"\\nDistribution for \\\"Spark\\\" : \",course_freq_dist.get(\"Spark\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleansing and Extraction"
      ],
      "metadata": {
        "id": "5kAD71HeYub_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 - Tokenization"
      ],
      "metadata": {
        "id": "aQYhDSosY2BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the base file into a raw text variable\n",
        "base_file = open(os.getcwd()+ \"/Spark-Course-Description.txt\", 'rt')\n",
        "raw_text = base_file.read()\n",
        "base_file.close()\n",
        "\n",
        "# Extract tokens\n",
        "token_list = nltk.word_tokenize(raw_text)\n",
        "print(\"Token List : \", token_list[:20])\n",
        "print(\"\\nTotal Tokens : \", len(token_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0osSd1mLY1gK",
        "outputId": "9095f7db-f5d2-475a-fdd7-aed63308b1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token List :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and']\n",
            "\n",
            "Total Tokens :  110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 - Cleansing Text\n",
        "We will see examples of removing punctuation and converting to lower case"
      ],
      "metadata": {
        "id": "4q8TNMk9ZeG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Punkt library to extract tokens\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "print(\"Token List after removing punctuation : \", token_list2[:20])\n",
        "print(\"\\nTotal tokens after removing punctuation : \", len(token_list2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJK5lZzVYq6Q",
        "outputId": "df0b323a-2ea2-4476-8372-026f2946c7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token List after removing punctuation :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'DevOps', 'specialists']\n",
            "\n",
            "Total tokens after removing punctuation :  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_list3 = [word.lower() for word in token_list2 ]\n",
        "print(\"Token list after converting to lower case : \", token_list3[:20])\n",
        "print(\"\\nTotal tokens after converting to lower case : \", len(token_list3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpW_H6XtZn9-",
        "outputId": "513d4696-9ae6-4a85-edbf-16853ccee1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token list after converting to lower case :  ['in', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'devops', 'specialists']\n",
            "\n",
            "Total tokens after converting to lower case :  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 - Stop word Removal\n",
        "Removing stop words by using a standard stop word list available in NLTK for English\n"
      ],
      "metadata": {
        "id": "sNv03GmhZj-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the standard stopword list\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Remove stopwords\n",
        "token_list4 = list(filter(lambda token: token not in stopwords.words('english'), token_list3))\n",
        "print(\"Token list after removing stop words : \", token_list4[:20])\n",
        "print(\"\\nTotal tokens after removing stop words : \", len(token_list4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xjbI4lVZhny",
        "outputId": "e3e21db8-b02e-40f5-fddc-df23fcaec5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token list after removing stop words :  ['order', 'construct', 'data', 'pipelines', 'networks', 'stream', 'process', 'store', 'data', 'data', 'engineers', 'data-science', 'devops', 'specialists', 'must', 'understand', 'combine', 'multiple', 'big', 'data']\n",
            "\n",
            "Total tokens after removing stop words :  62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 - Stemming"
      ],
      "metadata": {
        "id": "Fr0mOT7TZ2hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the PorterStemmer library for stemming.\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem data\n",
        "token_list5 = [stemmer.stem(word) for word in token_list4 ]\n",
        "print(\"Token list after stemming : \", token_list5[:20])\n",
        "print(\"\\nTotal tokens after Stemming : \", len(token_list5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdf9rPchZyDZ",
        "outputId": "8975a270-32bc-4139-941f-472b6150e9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token list after stemming :  ['order', 'construct', 'data', 'pipelin', 'network', 'stream', 'process', 'store', 'data', 'data', 'engin', 'data-sci', 'devop', 'specialist', 'must', 'understand', 'combin', 'multipl', 'big', 'data']\n",
            "\n",
            "Total tokens after Stemming :  62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05 - Lemmatization"
      ],
      "metadata": {
        "id": "nSXsGVXRaq8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the wordnet library to map words to their lemmatized form\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
        "print(\"Token list after Lemmatization : \", token_list6[:20])\n",
        "print(\"\\nTotal tokens after Lemmatization : \", len(token_list6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wsI7Mw6Z5tw",
        "outputId": "acfde62d-ec68-4742-f108-3ca4e2d4f553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token list after Lemmatization :  ['order', 'construct', 'data', 'pipeline', 'network', 'stream', 'process', 'store', 'data', 'data', 'engineer', 'data-science', 'devops', 'specialist', 'must', 'understand', 'combine', 'multiple', 'big', 'data']\n",
            "\n",
            "Total tokens after Lemmatization :  62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison of tokens between raw, stemming and lemmatization"
      ],
      "metadata": {
        "id": "_BjNfWfsayGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for token technlogies\n",
        "print( \"Raw : \", token_list4[20],\" , Stemmed : \", token_list5[20], \" , Lemmatized : \", token_list6[20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuIwi51XaxP-",
        "outputId": "6228bedc-50ea-4532-cd12-08256e58969c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw :  technologies  , Stemmed :  technolog  , Lemmatized :  technology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Text Processing"
      ],
      "metadata": {
        "id": "KjLt7eKefHT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 - Building N-grams"
      ],
      "metadata": {
        "id": "Qjq-VF5tfOm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "# Find bigrams and print the most common 5\n",
        "bigrams = ngrams(token_list6, 2)\n",
        "print(\"Most common bigrams : \")\n",
        "print(Counter(bigrams).most_common(5))\n",
        "\n",
        "# Find trigrams and print the most common 5\n",
        "trigrams = ngrams(token_list6,3)\n",
        "print(\" \\n Most common trigrams : \" )\n",
        "print(Counter(trigrams).most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSMKolP1fLpQ",
        "outputId": "1c621ed0-efd0-4f6d-c91b-7229900ff73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common bigrams : \n",
            "[(('big', 'data'), 3), (('data', 'pipeline'), 2), (('data', 'technology'), 2), (('apache', 'spark'), 2), (('order', 'construct'), 1)]\n",
            " \n",
            " Most common trigrams : \n",
            "[(('big', 'data', 'technology'), 2), (('order', 'construct', 'data'), 1), (('construct', 'data', 'pipeline'), 1), (('data', 'pipeline', 'network'), 1), (('pipeline', 'network', 'stream'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 - Parts-of-Speech Tagging"
      ],
      "metadata": {
        "id": "hDBleFKHfZK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the tagger package\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Tag and print the first 10 tokens\n",
        "nltk.pos_tag(token_list4)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2lEQKmifRHo",
        "outputId": "a9180022-103e-4047-cc00-635754f33e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('order', 'NN'),\n",
              " ('construct', 'NN'),\n",
              " ('data', 'NNS'),\n",
              " ('pipelines', 'NNS'),\n",
              " ('networks', 'NNS'),\n",
              " ('stream', 'VBP'),\n",
              " ('process', 'NN'),\n",
              " ('store', 'NN'),\n",
              " ('data', 'NNS'),\n",
              " ('data', 'NNS')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 - Building TF-IDF matrix"
      ],
      "metadata": {
        "id": "Iv-lqujkfnQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Use a small corpus for each visualization\n",
        "vector_corpus = [\n",
        "    'NBA is a Basketball league',\n",
        "    'Basketball is popular in America.',\n",
        "    'TV in America telecast BasketBall.',\n",
        "]\n",
        "\n",
        "# Create a vectorizer for english language\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Create the vector\n",
        "tfidf=vectorizer.fit_transform(vector_corpus)\n",
        "\n",
        "print(\"Tokens used as features are : \")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nSize of array. Each row represents a document. Each column represents a feature/token\")\n",
        "print(tfidf.shape)\n",
        "\n",
        "print(\"\\nActual TF-IDF array\")\n",
        "print(tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zjkJKo-fdGZ",
        "outputId": "7333de86-84ed-496e-9879-a5cd040d740a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens used as features are : \n",
            "['america' 'basketball' 'league' 'nba' 'popular' 'telecast' 'tv']\n",
            "\n",
            "Size of array. Each row represents a document. Each column represents a feature/token\n",
            "(3, 7)\n",
            "\n",
            "Actual TF-IDF array\n",
            "[[0.         0.38537163 0.65249088 0.65249088 0.         0.\n",
            "  0.        ]\n",
            " [0.54783215 0.42544054 0.         0.         0.72033345 0.\n",
            "  0.        ]\n",
            " [0.44451431 0.34520502 0.         0.         0.         0.5844829\n",
            "  0.5844829 ]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}